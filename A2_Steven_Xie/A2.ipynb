{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9c60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.3.0-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\spark-3.3.0-bin-hadoop3')\n",
    "import findspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b534ef",
   "metadata": {},
   "source": [
    "# Count the odd and even numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c11f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#instantiate the spark session\n",
    "spark = SparkSession.builder.appName(\"Titanic-Survival-Prediction\").getOrCreate()\n",
    "\n",
    "#set the shuffle partition same as number of cpu cores to improve performance \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4371bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a02304",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"integer.txt\"\n",
    "\n",
    "df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"header\", False) \\\n",
    "  .option(\"sep\", ',') \\\n",
    "  .option(\"path\", path) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a7da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|  _c0|\n",
      "+-----+\n",
      "|45687|\n",
      "| 5463|\n",
      "|34443|\n",
      "|  223|\n",
      "|  456|\n",
      "| 4667|\n",
      "|  234|\n",
      "| 9008|\n",
      "| 1234|\n",
      "| 2597|\n",
      "| 6253|\n",
      "|10399|\n",
      "| 3312|\n",
      "| 2175|\n",
      "|20087|\n",
      "| 6698|\n",
      "|31512|\n",
      "|  618|\n",
      "|11698|\n",
      "|13969|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc87a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|isOdd|count|\n",
      "+-----+-----+\n",
      "|    1|  496|\n",
      "|    0|  514|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.groupBy((col(\"_c0\")%2).alias(\"isOdd\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271221e",
   "metadata": {},
   "source": [
    "# Calculate the salary sum per department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da480c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = \"salary.txt\"\n",
    "\n",
    "df2 = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"header\", False) \\\n",
    "  .option(\"sep\", ' ') \\\n",
    "  .option(\"path\", path2) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09db40fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      _c0|  _c1|\n",
      "+---------+-----+\n",
      "|    Sales| 9136|\n",
      "| Research|13391|\n",
      "|Developer|22220|\n",
      "|       QA|31888|\n",
      "|Marketing|22215|\n",
      "|    Sales|45567|\n",
      "| Research| 4023|\n",
      "|Developer| 7262|\n",
      "|       QA| 5243|\n",
      "|Marketing|11425|\n",
      "|    Sales|11956|\n",
      "| Research|24149|\n",
      "|Developer|18258|\n",
      "|       QA|22962|\n",
      "|Marketing|28960|\n",
      "|    Sales| 8766|\n",
      "| Research|18343|\n",
      "|Developer|13686|\n",
      "|       QA|27626|\n",
      "|Marketing|32430|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d603c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|      _c0|sum(_c1)|\n",
      "+---------+--------+\n",
      "|    Sales| 3488491|\n",
      "|Developer| 3221394|\n",
      "|       QA| 3360624|\n",
      "|Marketing| 3158450|\n",
      "| Research| 3328284|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy('_c0').sum('_c1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ca8a7",
   "metadata": {},
   "source": [
    "# Implement MapReduce using Pyspark on file ‘shakespeare.txt’ download it from the Quercus. Show how many times these particular words appear in the document: Shakespeare, why, Lord, Library, GUTENBERG, WILLIAM, COLLEGE and WORLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e93b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "shakeRDD = spark.sparkContext.textFile(\"shakespeare-2.txt\", 4)\n",
    "times = shakeRDD.flatMap(lambda l: re.split(r'\\W+',l))\\\n",
    "                .map(lambda w: (w,1))\\\n",
    "                .reduceByKey(lambda x, y: x + y)\\\n",
    "                .filter(lambda t: t[0] in [\"Shakespeare\",\"Why\", \"Lord\", \"Library\", \"GUTENBERG\", \"WILLIAM\", \n",
    "                                           \"COLLEGE\", \"WORLD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99168ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shakespeare', 22),\n",
       " ('WILLIAM', 128),\n",
       " ('WORLD', 98),\n",
       " ('GUTENBERG', 100),\n",
       " ('COLLEGE', 98),\n",
       " ('Lord', 402),\n",
       " ('Library', 5),\n",
       " ('Why', 494)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13f0338",
   "metadata": {},
   "source": [
    "# 4. [Marks: 10] Calculate top 10 and bottom 10 words using file ‘shakespeare.txt’download it from the Quercus. Show 10 words with most count and 10 wordswith least count. You can limit by 10 in ascending and descending order ofcount. Show your code and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2a6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11412),\n",
       " ('I', 9714),\n",
       " ('and', 8942),\n",
       " ('of', 7968),\n",
       " ('to', 7742),\n",
       " ('a', 5796),\n",
       " ('you', 5360),\n",
       " ('my', 4922),\n",
       " ('in', 4803),\n",
       " ('d', 4365)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeRDD = spark.sparkContext.textFile(\"shakespeare-2.txt\", 4)\n",
    "words = shakeRDD.flatMap(lambda l: re.split(r'\\W+',l))\\\n",
    "                .map(lambda w: (w,1))\\\n",
    "                .reduceByKey(lambda x, y: x + y)\\\n",
    "                .filter(lambda t: t[0] != '')\\\n",
    "                .sortBy(lambda d: -d[1])\n",
    "words.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03764cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('www', 1),\n",
       " ('gutenberg', 1),\n",
       " ('Posting', 1),\n",
       " ('EBook', 1),\n",
       " ('Character', 1),\n",
       " ('encoding', 1),\n",
       " ('cooperation', 1),\n",
       " ('SHAREWARE', 1),\n",
       " ('PUBLIC', 1),\n",
       " ('DOMAIN', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakeRDD = spark.sparkContext.textFile(\"shakespeare-2.txt\", 4)\n",
    "words = shakeRDD.flatMap(lambda l: re.split(r'\\W+',l))\\\n",
    "                .map(lambda w: (w,1))\\\n",
    "                .reduceByKey(lambda x, y: x + y)\\\n",
    "                .filter(lambda t: t[0] != '')\\\n",
    "                .sortBy(lambda d: d[1])\n",
    "words.collect()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3599a",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0722ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527637f",
   "metadata": {},
   "source": [
    "# Calculate top 20 movies with highest ratings and top 15 users who provided highest ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b68ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4d6dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"movies.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"inferSchema\", True) \\\n",
    "  .option(\"header\", True) \\\n",
    "  .option(\"sep\", ',') \\\n",
    "  .option(\"path\", path) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91de4dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|rating|userId|\n",
      "+-------+------+------+\n",
      "|      2|     3|     0|\n",
      "|      3|     1|     0|\n",
      "|      5|     2|     0|\n",
      "|      9|     4|     0|\n",
      "|     11|     1|     0|\n",
      "|     12|     2|     0|\n",
      "|     15|     1|     0|\n",
      "|     17|     1|     0|\n",
      "|     19|     1|     0|\n",
      "|     21|     1|     0|\n",
      "|     23|     1|     0|\n",
      "|     26|     3|     0|\n",
      "|     27|     1|     0|\n",
      "|     28|     1|     0|\n",
      "|     29|     1|     0|\n",
      "|     30|     1|     0|\n",
      "|     31|     1|     0|\n",
      "|     34|     1|     0|\n",
      "|     37|     1|     0|\n",
      "|     41|     2|     0|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a3f3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieID|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|     32|2.9166666666666665|\n",
      "|     90|            2.8125|\n",
      "|     30|               2.5|\n",
      "|     94| 2.473684210526316|\n",
      "|     23| 2.466666666666667|\n",
      "|     49|            2.4375|\n",
      "|     18|               2.4|\n",
      "|     29|               2.4|\n",
      "|     52| 2.357142857142857|\n",
      "|     62|              2.25|\n",
      "|     53|              2.25|\n",
      "|     92|2.2142857142857144|\n",
      "|     46|               2.2|\n",
      "|     68|2.1578947368421053|\n",
      "|     87|2.1333333333333333|\n",
      "|      2|2.1052631578947367|\n",
      "|     69| 2.076923076923077|\n",
      "|     27| 2.066666666666667|\n",
      "|     88|2.0555555555555554|\n",
      "|     22|              2.05|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"movieID\").avg('rating').sort(\"avg(rating)\", ascending = False).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33113dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|userId|       avg(rating)|\n",
      "+------+------------------+\n",
      "|    11|2.2857142857142856|\n",
      "|    26| 2.204081632653061|\n",
      "|    22|2.1607142857142856|\n",
      "|    23|2.1346153846153846|\n",
      "|     2|2.0652173913043477|\n",
      "|    17|1.9565217391304348|\n",
      "|     8|1.8979591836734695|\n",
      "|    24|1.8846153846153846|\n",
      "|    12|1.8545454545454545|\n",
      "|     3|1.8333333333333333|\n",
      "|    29| 1.826086956521739|\n",
      "|    28|              1.82|\n",
      "|     9|1.7924528301886793|\n",
      "|    14|1.7894736842105263|\n",
      "|    16|1.7777777777777777|\n",
      "+------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"userId\").avg('rating').sort(\"avg(rating)\", ascending = False).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef385c64",
   "metadata": {},
   "source": [
    "# Split dataset into train and test. (80,20 and 70,30) use collaborativefiltering approach on 70 percent of your data and test with the other 30 percent and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18a6833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train2, test2 = df.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "alsdata = ALS(userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\",\n",
    "               coldStartStrategy = \"drop\")\n",
    "model1 = alsdata.fit(train1)\n",
    "model2 = alsdata.fit(train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13bdf0",
   "metadata": {},
   "source": [
    "# Compare and evaluate both of your models with evaluation metrics (RMSE or MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ea516a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1136784395132724\n",
      "1.0176715586526854\n"
     ]
    }
   ],
   "source": [
    "eva_MSE = RegressionEvaluator(metricName = \"mse\", predictionCol=\"prediction\", labelCol=\"rating\")\n",
    "pre1 = model1.transform(test1)\n",
    "pre2 = model2.transform(test2)\n",
    "\n",
    "mse1 = eva_MSE.evaluate(pre1)\n",
    "mse2 = eva_MSE.evaluate(pre2)\n",
    "print(mse1)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2b03994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0553096415333618\n",
      "1.00879708497432\n"
     ]
    }
   ],
   "source": [
    "eva_RMSE = RegressionEvaluator(metricName = \"rmse\", predictionCol=\"prediction\", labelCol=\"rating\")\n",
    "\n",
    "rmse1 = eva_RMSE.evaluate(pre1)\n",
    "rmse2 = eva_RMSE.evaluate(pre2)\n",
    "print(rmse1)\n",
    "print(rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51830ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575173862548971\n",
      "0.703591212059242\n"
     ]
    }
   ],
   "source": [
    "eva_MAE = RegressionEvaluator(metricName = \"mae\", predictionCol=\"prediction\", labelCol=\"rating\")\n",
    "\n",
    "mae1 = eva_MAE.evaluate(pre1)\n",
    "mae2 = eva_MAE.evaluate(pre2)\n",
    "print(mae1)\n",
    "print(mae2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae36e37",
   "metadata": {},
   "source": [
    "# tune the parameters of your algorithm to get the best set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d2bacf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0459846864560232\n"
     ]
    }
   ],
   "source": [
    "para = ParamGridBuilder()\\\n",
    "        .addGrid(alsdata.regParam, [0.1, 1.0])\\\n",
    "        .addGrid(alsdata.maxIter, [10, 15])\\\n",
    "        .build()\n",
    "\n",
    "CV = CrossValidator(estimator = alsdata, estimatorParamMaps = para, evaluator = eva_MSE, numFolds = 7)\n",
    "\n",
    "model1 = CV.fit(train1)\n",
    "best1 = model1.bestModel\n",
    "pre1 = best1.transform(test1)\n",
    "\n",
    "mse1 = eva_MSE.evaluate(pre1)\n",
    "print(mse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fa1463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0224152006252087\n"
     ]
    }
   ],
   "source": [
    "model2 = CV.fit(train2)\n",
    "best2 = model2.bestModel\n",
    "pre2 = best2.transform(test2)\n",
    "\n",
    "mse2 = eva_MSE.evaluate(pre2)\n",
    "print(mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36454acf",
   "metadata": {},
   "source": [
    "# Top 15 movies recommendations for user id 10 and user id 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e33fb7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+\n",
      "|userId|movieId|   rating|\n",
      "+------+-------+---------+\n",
      "|    10|     92| 2.624387|\n",
      "|    10|     12|2.5440228|\n",
      "|    10|      9|2.2091775|\n",
      "|    10|     62|2.2039583|\n",
      "|    10|     81|2.1534595|\n",
      "|    10|     91| 2.139664|\n",
      "|    10|     46|1.9177157|\n",
      "|    10|     71|1.8875595|\n",
      "|    10|     82|1.7437273|\n",
      "|    10|     88|1.7228769|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recom = best1.recommendForAllUsers(20)\n",
    "n = recom.withColumn(\"rec_exp\", explode(\"recommendations\"))\\\n",
    "         .select(\"userId\", col(\"rec_exp.movieId\"), col(\"rec_exp.rating\"))\n",
    "user10 = df.filter(df.userId == 10).select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "n.filter(n.userId == 10).filter(n.movieId.isin(user10) == False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8025bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+\n",
      "|userId|movieId|   rating|\n",
      "+------+-------+---------+\n",
      "|    14|     85|3.3395948|\n",
      "|    14|     58| 3.142126|\n",
      "|    14|     43| 2.784018|\n",
      "|    14|      2|2.6485991|\n",
      "|    14|     60|2.4759626|\n",
      "|    14|     74|2.4675732|\n",
      "|    14|     41| 2.311631|\n",
      "|    14|     77|2.2214978|\n",
      "+------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user14 = df.filter(df.userId == 14).select(\"movieId\").rdd.flatMap(lambda x: x).collect()\n",
    "n.filter(n.userId == 14).filter(n.movieId.isin(user14) == False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823b1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
